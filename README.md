# War and Prejudice

## Introduction

This report investigates alternate forms of input for a transformer language model. We first describe a method for vector encoding of vocabulary. Using this vocabulary we train a basic transformer model, and compare it to a similar model trained on a simple one-hot encoded vocabulary.

## Vector Embedding

- used cbow because
- used vocab size because
- used embedding dimension because
- used windows size because
- ...

examine tSNE

maybe suggest improvements

## Transformer Model

- model architecture
- training paramters
- one hot results
- tok2vec results

Ablation?

## Conclusion

- recap paramters
- and architecture
- recap results
