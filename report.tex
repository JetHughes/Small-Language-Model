% LaTeX Template for short student reports.
% Citations should be in bibtex format and go in references.bib
\documentclass[a4paper, 11pt]{article}
\usepackage[top=3cm, bottom=3cm, left = 2cm, right = 2cm]{geometry} 
\geometry{a4paper} 
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{graphicx} 
\usepackage{amsmath,amssymb}  
\usepackage{bm}  
\usepackage[pdftex,bookmarks,colorlinks,breaklinks]{hyperref}  
%\hypersetup{linkcolor=black,citecolor=black,filecolor=black,urlcolor=black} % black links, for printed output
\usepackage{memhfixc} 
\usepackage{pdfsync}  
\usepackage{fancyhdr}

% customised packages
\usepackage{listings}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}

% title info
\title{Transformer Based Language Model}
\author{Jet Hughes}
\date{}

\begin{document}
\maketitle
% \newpage

% \tableofcontents
% \newpage

\section{Introduction}
This report investigates alternate forms of input for a transformer language model. We first describe a method for vector embedding of vocabulary. Using this vocabulary embedding we train a train a transformer model, and compare it to a similar mode trained on one-hot encoded vocabulary. We find that ...
\section{Vector Embedding}
\subsection{Architecture and Training}
For this assigment I used a Continuous Bag of Words (CBOW) model to generate a vector embedding of the vocabulary. The CBOW model is a simple neural network that takes a window of tokens as input and predicts the center token. The number of neurons in the input and output layers is equal to the size of the vocabulary. The number of neurons in the hidden layer is the size of the embedding. Each row of the weight matrix between the input and hidden layer is the embedding of a token in the vocabulary. The model is trained on the entirety of Tolstoy's war and peace. We used a vocabulary size of 1000, a window size of 8, and the dimension of the embedding was 100. The model was trained for 10 epochs with an adaptive learning rate using categorical cross entropy loss.

\subsection{Results}
A t-SNE plot of 100 tokens from the embedding is shown in figure \ref{fig:embedding}. The embedding appears to have some structure, with similar tokens being grouped together. For example, the names of characters are grouped together, and so are words such as "uncle", "aunt", "father", "sister", etc.

\section{Transformer}

\section{Conclusion}

% \bibliographystyle{abbrv}
% \bibliography{references}


\end{document}